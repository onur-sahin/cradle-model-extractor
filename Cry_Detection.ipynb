{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from random import randint\n",
    "import torch\n",
    "\n",
    "# @staticmethod\n",
    "def pad_trunc(aud, max_ms:int):\n",
    "\n",
    "    sig, sr = aud\n",
    "\n",
    "    if sig.dim() > 1: # if there are more than one channels\n",
    "\n",
    "        num_channel, sig_len = sig.shape\n",
    "\n",
    "        max_len = sr//1000*max_ms\n",
    "\n",
    "        if (sig_len > max_len):\n",
    "            sig = sig[:, :max_len].to(device)\n",
    "\n",
    "        elif(sig_len < max_len):\n",
    "\n",
    "            pad_begin_len = randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "            pad_begin = torch.zeros((num_channel, pad_begin_len), device=device)\n",
    "            pad_end = torch.zeros((num_channel, pad_end_len), device=device)\n",
    "\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), dim=1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    elif sig.dim()==1:             # if there is only one channel\n",
    "\n",
    "        sig_len = sig.shape[0]\n",
    "        max_len = sr//1000*max_ms\n",
    "\n",
    "        if (sig_len > max_len):\n",
    "            sig = sig[:max_len].to(device)\n",
    "\n",
    "        elif(sig_len < max_len):\n",
    "\n",
    "            pad_begin_len = randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "            pad_begin = torch.zeros(pad_begin_len, device=device)\n",
    "            pad_end = torch.zeros(pad_end_len, device=device)\n",
    "\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), dim=0).to(device)\n",
    "\n",
    "    \n",
    "    return (sig, sr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# @staticmethod\n",
    "def time_shift(aud, shift_limit):\n",
    "    sig, sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    sig = sig.roll(shift_amt).to(device)\n",
    "    return (sig, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @staticmethod\n",
    "import torch\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def rechannel(aud, n_new_channel):\n",
    "    \n",
    "    sig, sr = aud\n",
    "\n",
    "    #if new channel count equals old channel count is'nt\n",
    "    if(sig.dim() == 1 ):\n",
    "        n_current_channel = 1\n",
    "\n",
    "\n",
    "    #detect current channel count\n",
    "    elif(sig.dim() > 1 ):\n",
    "        n_current_channel = sig.shape[0]\n",
    "\n",
    "    if (n_current_channel == n_new_channel):\n",
    "        return aud\n",
    "\n",
    "\n",
    "    #new channel count required is greater than current channel count\n",
    "    elif(n_new_channel > n_current_channel):\n",
    "        \n",
    "        dif = n_new_channel - n_current_channel\n",
    "\n",
    "        new_sig = sig[ randint(0, n_current_channel-1) ]\n",
    "        new_sig = torch.unsqueeze(new_sig, dim=0)\n",
    "        \n",
    "        for i in range(dif-1):\n",
    "            temp_sig = sig[ randint(0, n_current_channel-1) ]\n",
    "            temp_sig = torch.unsqueeze(temp_sig, dim=0)\n",
    "            \n",
    "            new_sig = torch.cat((new_sig, temp_sig), dim=0)\n",
    "\n",
    "        resig = torch.cat((sig, new_sig), dim= 0)\n",
    "        \n",
    "        \n",
    "\n",
    "    elif n_new_channel < n_current_channel :\n",
    "\n",
    "        n_mix_channel = n_current_channel - n_new_channel + 1\n",
    "\n",
    "        mix_begin = randint(0, n_current_channel-n_mix_channel)\n",
    "        mix_end = mix_begin + n_mix_channel\n",
    "\n",
    "        mix_aud = torch.mean( sig[mix_begin:mix_end], dim=0 )\n",
    "        mix_aud = torch.unsqueeze(mix_aud, dim=0)\n",
    "        \n",
    "        resig = torch.cat([sig[0:mix_begin,:], mix_aud, sig[mix_end:,:]]).to(device)     \n",
    "        \n",
    "    return ((resig, sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000, device='cuda:0')\n",
      "tensor([1., 2., 3., 4., 1., 2., 3., 4.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1.,2,3,4]).to(device)\n",
    "# b = torch.t_copy(a)\n",
    "# b = a.t_copy()\n",
    "b = torch.t_copy(a)\n",
    "k = torch.cat((a,b))\n",
    "b[0] =17\n",
    "print(a.mean())\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import torchaudio.transforms as transforms\n",
    "\n",
    "\n",
    "# @staticmethod\n",
    "def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    \n",
    "    sig, sr = aud\n",
    "    top_db = 80\n",
    "    \n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels).to(device)(sig)\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db).to(device)(spec)\n",
    "    \n",
    "    return (spec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @staticmethod\n",
    "def spectro_augment(spec, max_mask_pct=0.1, n_freq_mask=1, n_time_mask=1):\n",
    "    _, n_mels, n_steps =spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_masks_param = max_mask_pct * n_mels\n",
    "    \n",
    "    for _ in range(n_freq_mask):\n",
    "        aug_spec = transforms.FrequencyMasking(freq_masks_param)(aug_spec, mask_value)\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "\n",
    "    for _ in range(n_time_mask):\n",
    "        aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATA FRAME FOR DATASET\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def create_df():\n",
    "\n",
    "    PATH = './datasets/data/'\n",
    "\n",
    "    \n",
    "    sound_paths=[]\n",
    "    sound_classes = []\n",
    "\n",
    "    for folderName in glob.glob(PATH+'*'):\n",
    "    \n",
    "        class_name = os.path.basename(folderName)\n",
    "\n",
    "        for sound_path in glob.glob(PATH+\"/\"+class_name+'/*'):\n",
    "            \n",
    "            sound_paths.append(sound_path)\n",
    "            sound_classes.append(class_name)\n",
    "       \n",
    "\n",
    "    return pd.DataFrame({\"file_path\":sound_paths, \"class\":sound_classes})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATASET CLASS\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "class SoundDS(Dataset):\n",
    "\n",
    "    def __init__(   self,\n",
    "                    df,\n",
    "                    sr=22050,\n",
    "                    duration=None,\n",
    "                    n_channel=1,\n",
    "                    shift_pct=0.4,\n",
    "                    max_mask_pct=0.1,\n",
    "                    n_freq_mask=1,\n",
    "                    n_time_mask=1\n",
    "                ):\n",
    "        \n",
    "        self.df = df\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.n_channel = n_channel\n",
    "        self.shift_pct = shift_pct\n",
    "        self.max_mask_pct = max_mask_pct\n",
    "        self.n_freq_mask = n_freq_mask\n",
    "        self.n_time_mask = n_time_mask\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.classes = set()\n",
    "        self.classes.update(self.df['class'].to_list())\n",
    "        self.classes = [ x for x in self.classes]\n",
    "        self.classes.sort()\n",
    "        print(self.classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        file_path = self.df.loc[index, \"file_path\"]\n",
    "        class_name = self.df.loc[index, \"class\"]\n",
    "        class_num = self.classes.index(class_name)\n",
    "\n",
    "\n",
    "        aud, sr = librosa.load( file_path,\n",
    "                                sr=self.sr,\n",
    "                                mono=self.n_channel==1,\n",
    "                                duration=self.duration)\n",
    "                                \n",
    "        aud = torch.from_numpy(aud).to(device)\n",
    "\n",
    "        if aud.dim() == 1:\n",
    "            aud = torch.unsqueeze(aud, dim=0).to(device)\n",
    "        \n",
    "        sig = (aud, sr)\n",
    "\n",
    "        sig = pad_trunc(sig, self.duration)\n",
    "\n",
    "        sig = time_shift(sig, self.shift_pct)\n",
    "\n",
    "        sig = rechannel(sig, self.n_channel)\n",
    "\n",
    "        sig = spectro_gram(sig, n_mels=64, n_fft=1024, hop_len=None)\n",
    "\n",
    "        sig = spectro_augment(sig,\n",
    "                              max_mask_pct=self.max_mask_pct,\n",
    "                              n_freq_mask=self.n_freq_mask,\n",
    "                              n_time_mask=self.n_time_mask)\n",
    "\n",
    "\n",
    "        return sig.to(device), class_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = iter(train_dl)\n",
    "# k = p.next()\n",
    "# print(model(k[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A CONVOLUTIONAL MODEL CLASS\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channel):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        conv_layers = []\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # First convolution block\n",
    "        self.conv1 = nn.Conv2d(n_channel, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        # self.relu layer\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers.extend( [self.conv1, self.relu, self.bn1] )\n",
    "\n",
    "        # Second convolution block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        # self.relu layer\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers.extend( [self.conv2, self.relu, self.bn2] )\n",
    "\n",
    "\n",
    "        # Third convolution block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        #self.relu layer\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a= 0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers.extend( [self.conv3, self.relu, self.bn3] )\n",
    "\n",
    "        # Forth convolution block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        #self.relu layer\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a = 0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers.extend( [self.conv4, self.relu, self.bn4] )\n",
    "\n",
    "        # Last step for classification\n",
    "        # Fully Connected Linear Layer\n",
    "\n",
    "        # Downsizing with pooling\n",
    "        self.avgPool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        \n",
    "        #flattening for input to linear layer\n",
    "        self.flatten = nn.Flatten(1, -1)\n",
    "\n",
    "        self.linear_layer = nn.Linear(in_features=64, out_features=4)\n",
    "\n",
    "        conv_layers.extend([self.avgPool, self.flatten, self.linear_layer])\n",
    "\n",
    "        #The list of conv_layer is unpacked and sent\n",
    "        self.conv = nn.Sequential(*conv_layers) # nn.Squential() gets *args or OrderedDict\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        return self.conv(inputs)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x = torch.empty((16,2,64,215), dtype=torch.float32)\n",
    "# # x.normal_()\n",
    "\n",
    "# x = librosa.load('./datasets/data/301 - Crying baby/1-22694-A.ogg')\n",
    "# x = torch.from_numpy(x[0])\n",
    "# # x = torch.unsqueeze(x[0], dim=0)\n",
    "# x.shape\n",
    "# # y=model(x)\n",
    "\n",
    "# # print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['301 - Crying baby', '901 - Silence', '902 - Noise', '903 - Baby laugh']\n"
     ]
    }
   ],
   "source": [
    "# PREPAIRING DATALOADER\n",
    "\n",
    "from random import shuffle\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataframe = create_df()\n",
    "\n",
    "audio_dataset = SoundDS(\n",
    "                        df=dataframe,\n",
    "                        sr=22050,\n",
    "                        duration=5000,\n",
    "                        n_channel=1,\n",
    "                        shift_pct=0.4,\n",
    "                        max_mask_pct=0.1,\n",
    "                        n_freq_mask=1,\n",
    "                        n_time_mask=1\n",
    "                        )\n",
    "\n",
    "num_item = len(audio_dataset)\n",
    "num_train = round(num_item*0.8)\n",
    "num_test = num_item - num_train\n",
    "\n",
    "train_ds, test_ds = random_split(audio_dataset, [num_train, num_test] )\n",
    "\n",
    "train_dl = DataLoader(train_ds,batch_size=8, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE A FUNCTIN FOR MODEL TRAINING\n",
    "\n",
    "from sched import scheduler\n",
    "import torch\n",
    "import os\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from ignite.handlers import create_lr_scheduler_with_warmup\n",
    "\n",
    "\n",
    " #create folde for saving checkpoints\n",
    "if not os.path.exists('./check_points_for_detection'):\n",
    "    os.mkdir('./check_points_for_detection')\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_dl, num_epoch, lr):\n",
    "\n",
    "    writer = SummaryWriter('./runs/Cry_Detection/writer')\n",
    "    writer2 = SummaryWriter('./runs/Cry_Detection/writer2')\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # torch_lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "    #                                                     mode='min',\n",
    "    #                                                     factor=0.75,\n",
    "    #                                                     patience=50)\n",
    "\n",
    "    torch_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer,\n",
    "                                                    gamma=0.98)\n",
    "\n",
    "    # scheduler = create_lr_scheduler_with_warmup(torch_lr_scheduler,\n",
    "    #                                         warmup_start_value=0,\n",
    "    #                                         warmup_duration=3,\n",
    "    #                                         warmup_end_value=0.01)\n",
    "\n",
    "    num_batches = len(train_dl)\n",
    "\n",
    "    # scheduler(None)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        running_loss       = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction   = 0\n",
    "\n",
    "        last_4_loss = 0.0\n",
    "        last_4_correct_pred = 0\n",
    "        last_4_pred = 0\n",
    "\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        for i, data in enumerate(train_dl):\n",
    "\n",
    "            torch.set_grad_enabled(False)\n",
    "\n",
    "            inputs:torch.Tensor = data[0].to(device)\n",
    "            labels:torch.Tensor = data[1].to(device)\n",
    "\n",
    "            mean:torch.Tensor = inputs.mean()\n",
    "            std:torch.Tensor  = inputs.std()\n",
    "\n",
    "            inputs = Normalize(mean, std)(inputs)  # OR inputs = (inputs - mean) / std\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            torch.set_grad_enabled(True)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss:torch.Tensor = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.mean().item()\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            acc = (predictions==labels).sum().item()/predictions.shape[0]\n",
    "            correct_prediction += (predictions==labels).sum().item()\n",
    "            total_prediction += predictions.shape[0]\n",
    "\n",
    "            last_4_loss += loss.item()\n",
    "            last_4_correct_pred += (predictions==labels).sum().item()\n",
    "            last_4_pred += predictions.shape[0] \n",
    "\n",
    "            count += 1\n",
    "            if count%4 == 0 or count == 1:\n",
    "\n",
    "                torch_lr_scheduler.step()\n",
    "                \n",
    "                writer.add_scalar('Training loss - Average of Last 4 Steps', last_4_loss/4, (epoch*len(train_dl)+i+1))\n",
    "                \n",
    "                writer.add_scalar('Accuracy - Average of Last 4 steps',\n",
    "                                   last_4_correct_pred/last_4_pred,\n",
    "                                   (epoch*len(train_dl)+i+1))\n",
    "                \n",
    "                writer.add_scalars('Loss and Accuracy - Average of Last 4 Steps',\n",
    "                                  {'Accuracy': last_4_correct_pred/last_4_pred,\n",
    "                                   'Loss': last_4_loss/4}, (epoch*len(train_dl)+i+1))\n",
    "\n",
    "                last_4_loss = 0\n",
    "                last_4_correct_pred= 0\n",
    "                last_4_pred = 0\n",
    "                \n",
    "            writer.add_scalar(\"Learning Rate\", torch_lr_scheduler.get_last_lr()[0], (epoch*len(train_dl)+i+1))\n",
    "            writer.add_scalar(\"Steps - Epochs\", len(train_dl)*epoch + i + 1 , epoch)\n",
    "\n",
    "        avg_loss = running_loss / num_batches\n",
    "        avg_acc = correct_prediction / total_prediction\n",
    "\n",
    "        \n",
    "        writer2.add_scalar('Training loss - Average of Last Epoch', avg_loss, (epoch+1))\n",
    "        writer2.add_scalar('Accuracy - Average Last epoch', avg_acc, (epoch+1))\n",
    "        writer2.add_scalars('Comparison - Loss and Accuracy', {'Accuracy': avg_acc, 'Loss': avg_loss}, (epoch+1))\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}, Loss: {avg_loss:.2f}, Accuracy: {avg_acc:.2f}')\n",
    "\n",
    "        path = \"./check_points_for_detection/checkpoint_\"+str(epoch+1)+\".pth\"\n",
    "\n",
    "        checkPoint = {\"model_state\": model.state_dict(),\n",
    "                      \"optim_state\": optimizer.state_dict,\n",
    "                      \"scheduler\"  : torch_lr_scheduler.state_dict()}\n",
    "\n",
    "        torch.save(checkPoint, path)\n",
    "\n",
    "    \n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.65, Accuracy: 0.76\n",
      "Epoch: 2, Loss: 0.53, Accuracy: 0.81\n",
      "Epoch: 3, Loss: 0.44, Accuracy: 0.86\n",
      "Epoch: 4, Loss: 0.29, Accuracy: 0.90\n",
      "Epoch: 5, Loss: 0.30, Accuracy: 0.90\n",
      "Epoch: 6, Loss: 0.27, Accuracy: 0.90\n",
      "Epoch: 7, Loss: 0.22, Accuracy: 0.92\n",
      "Epoch: 8, Loss: 0.23, Accuracy: 0.93\n",
      "Epoch: 9, Loss: 0.21, Accuracy: 0.93\n",
      "Epoch: 10, Loss: 0.15, Accuracy: 0.96\n",
      "Epoch: 11, Loss: 0.15, Accuracy: 0.95\n",
      "Epoch: 12, Loss: 0.19, Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# CREATE A MODEL AND TRAIN IT\n",
    "\n",
    "import torch.utils.tensorboard\n",
    "\n",
    "n_channel = audio_dataset.n_channel\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model         = Classifier(n_channel).to(device)\n",
    "num_epoch     = 12\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Train model\n",
    "train_model(model, train_dl, num_epoch, learning_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE MODEL TEST FUNCTION\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def test_model(model:nn.Module, test_dl:DataLoader):\n",
    "\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    writer = SummaryWriter('./runs/Cry_DetectionTest')\n",
    "   \n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.eval()    \n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_batches = len(test_dl)\n",
    "        \n",
    "    total_loss       = 0.0\n",
    "    total_correct_prediction = 0\n",
    "    total_prediction   = 0\n",
    "\n",
    "    for i, data in enumerate(test_dl):\n",
    "\n",
    "        batch_size = len(data[0])\n",
    "\n",
    "        inputs:torch.Tensor = data[0].to(device)\n",
    "        labels:torch.Tensor = data[1].to(device)\n",
    "\n",
    "        mean:torch.Tensor = inputs.mean()\n",
    "        std:torch.Tensor  = inputs.std()\n",
    "\n",
    "        inputs = Normalize(mean, std)(inputs)  # OR inputs = (inputs - mean) / std\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss:torch.Tensor = criterion(outputs, labels)\n",
    "\n",
    "        total_loss += loss.mean().item()\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        acc = (predictions==labels).sum().item()/predictions.shape[0]\n",
    "        total_correct_prediction += (predictions==labels).sum().item()\n",
    "        total_prediction += predictions.shape[0]\n",
    "\n",
    "            \n",
    "        writer.add_scalar('Test loss - Average of Every Batch', loss.item()/batch_size, i+1)\n",
    "        writer.add_scalar('Test Accuracy - Average of Every Batch', acc, i+1)\n",
    "        \n",
    "        print(f'Bacth: {i+1}, Loss: {loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_acc = total_correct_prediction / total_prediction\n",
    "    \n",
    "\n",
    "       \n",
    "    print(f'Test Finised:\\nLoss: {avg_loss:.2f}, Accuracy: {avg_acc:.2f}')\n",
    "        \n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bacth: 1, Loss: 0.02, Accuracy: 1.00\n",
      "Bacth: 2, Loss: 0.02, Accuracy: 1.00\n",
      "Bacth: 3, Loss: 0.05, Accuracy: 1.00\n",
      "Bacth: 4, Loss: 0.26, Accuracy: 0.90\n",
      "Bacth: 5, Loss: 0.04, Accuracy: 1.00\n",
      "Bacth: 6, Loss: 0.03, Accuracy: 1.00\n",
      "Bacth: 7, Loss: 0.25, Accuracy: 0.90\n",
      "Bacth: 8, Loss: 0.37, Accuracy: 0.90\n",
      "Bacth: 9, Loss: 0.00, Accuracy: 1.00\n",
      "Test Finised:\n",
      "Loss: 0.11, Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# TEST MODEL\n",
    "\n",
    "import torch\n",
    "\n",
    "modelForTest = Classifier(1)\n",
    "path = './check_points_for_detection/checkpoint_10.pth'\n",
    "device = torch.device(\"cuda:0\")\n",
    "modelForTest.load_state_dict( torch.load(path, map_location=device)['model_state'] )\n",
    "\n",
    "test_model(modelForTest, test_dl)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "w = torch.empty(3, 5)\n",
    "print(w)\n",
    "nn.init.kaiming_normal_(w, mode='fan_in', nonlinearity='relu')\n",
    "print(w.data.zero_())\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, i in enumerate( train_dl):\n",
    "   print(k, i[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "tnsr = torch.tensor(  [ [\n",
    "                        [1.,2,3,4],\n",
    "                        [6,7,8,9],\n",
    "                        [11,12,13,14],\n",
    "                        ],\n",
    "                        [\n",
    "                        [1.,2,3,4],\n",
    "                        [6,7,8,9],\n",
    "                        [11,12,13,14],\n",
    "                        ]\n",
    "                        \n",
    "                        ])\n",
    "\n",
    "\n",
    "m = tnsr.mean()\n",
    "s = tnsr.std()\n",
    "print(m, s)\n",
    "\n",
    "torchvision.transforms.Normalize(s, m)(tnsr)\n",
    "\n",
    "print(tnsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [1,3,4]\n",
    "b= [1,2,3]\n",
    "a.extend([4,4,4,4,4,44,4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "b = {\"a\":1, \"b\":2, \"c\":3}\n",
    "\n",
    "\n",
    "\n",
    "def fnc(**arg):\n",
    "    for i in arg.items():\n",
    "        print(i)\n",
    "\n",
    "fnc(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ecc031d86696929f30629648f7816eb3c79c248c1db74fa715074cbd38220b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is for showing the spectrogram of an audio sample loaded using Torchaudio,\n",
    "# it doesn't work for samples that are loaded using Librosa.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import torchaudio\n",
    "\n",
    "#this auto implement\n",
    "\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\"):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "import librosa.display\n",
    "import torchaudio.transforms as transforms\n",
    "\n",
    "# @staticmethod\n",
    "def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    \n",
    "    sig, sr = aud\n",
    "    top_db = 80\n",
    "    \n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    \n",
    "    return (spec)\n",
    "\n",
    "# aud = torchaudio.load(\"./yeni.wav\")\n",
    "\n",
    "# spec = spectro_gram(aud)\n",
    "\n",
    "# plot_spectrogram(spec[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @staticmethod\n",
    "import torch\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def rechannel(aud, n_new_channel):\n",
    "    \n",
    "    sig, sr = aud\n",
    "\n",
    "    #if new channel count equals old channel count\n",
    "    if(sig.dim() == 1 ):\n",
    "        n_current_channel = 1\n",
    "\n",
    "\n",
    "    #detect current channel count\n",
    "    elif(sig.dim() > 1 ):\n",
    "        n_current_channel = sig.shape[0]\n",
    "\n",
    "    if (n_current_channel == n_new_channel):\n",
    "        return aud\n",
    "\n",
    "\n",
    "    #new channel count required is greater than the current channel count\n",
    "    elif(n_new_channel > n_current_channel):\n",
    "        \n",
    "        dif = n_new_channel - n_current_channel\n",
    "\n",
    "        new_sig = sig[ randint(0, n_current_channel-1) ]\n",
    "        new_sig = torch.unsqueeze(new_sig, dim=0)\n",
    "        \n",
    "        for i in range(dif-1):\n",
    "            temp_sig = sig[ randint(0, n_current_channel-1) ]\n",
    "            temp_sig = torch.unsqueeze(temp_sig, dim=0)\n",
    "            \n",
    "            new_sig = torch.cat((new_sig, temp_sig), dim=0)\n",
    "\n",
    "        resig = torch.cat((sig, new_sig), dim= 0)\n",
    "        \n",
    "        \n",
    "\n",
    "    elif n_new_channel < n_current_channel :\n",
    "\n",
    "        n_mix_channel = n_current_channel - n_new_channel + 1\n",
    "\n",
    "        mix_begin = randint(0, n_current_channel-n_mix_channel)\n",
    "        mix_end = mix_begin + n_mix_channel\n",
    "\n",
    "        mix_aud = torch.mean( sig[mix_begin:mix_end], dim=0 )\n",
    "        mix_aud = torch.unsqueeze(mix_aud, dim=0)\n",
    "        \n",
    "        resig = torch.cat([sig[0:mix_begin,:], mix_aud, sig[mix_end:,:]])      \n",
    "\n",
    "    return ((resig, sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @staticmethod\n",
    "# Bu fonksiyona gerek yok çünkü torchaudio.transforms.Resample kendisi birden fazla kanallı ses örneklerini\n",
    "# alabiliyor ve sesin frekansını değiştirdikten sonra aynı kanal sayısı ile örneği geri döndürüyor\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "def resample(aud, newsr):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if( sr == newsr):\n",
    "        return aud\n",
    "\n",
    "    num_channels = sig.shape[0]\n",
    "\n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "\n",
    "    if(num_channels > 1):\n",
    "        retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "        resig = torch.cat([resig, retwo])\n",
    "\n",
    "    return ((resig, newsr))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from random import randint\n",
    "\n",
    "# @staticmethod\n",
    "def pad_trunc(aud, max_ms:int):\n",
    "\n",
    "    sig, sr = aud\n",
    "\n",
    "    if sig.dim() > 1: # if there are more than one channel\n",
    "\n",
    "        num_channel, sig_len = sig.shape\n",
    "\n",
    "        max_len = sr//1000*max_ms\n",
    "\n",
    "        if (sig_len > max_len):\n",
    "            sig = sig[:, :max_len]\n",
    "\n",
    "        elif(sig_len < max_len):\n",
    "\n",
    "            pad_begin_len = randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "            pad_begin = torch.zeros((num_channel, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_channel, pad_end_len))\n",
    "\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    elif sig.dim()==1:             # if there is only one channel\n",
    "\n",
    "        sig_len = sig.shape[0]\n",
    "        max_len = sr//1000*max_ms\n",
    "\n",
    "        if (sig_len > max_len):\n",
    "            sig = sig[:max_len]\n",
    "\n",
    "        elif(sig_len < max_len):\n",
    "\n",
    "            pad_begin_len = randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "            pad_begin = torch.zeros(pad_begin_len)\n",
    "            pad_end = torch.zeros(pad_end_len)\n",
    "\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), dim=0)\n",
    "\n",
    "    \n",
    "    return (sig, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# @staticmethod\n",
    "def time_shift(aud, shift_limit):\n",
    "    sig, sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import torchaudio.transforms as transforms\n",
    "\n",
    "# @staticmethod\n",
    "def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    \n",
    "    sig, sr = aud\n",
    "    top_db = 80\n",
    "    \n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    \n",
    "    return (spec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @staticmethod\n",
    "def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps =spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_masks_param = max_mask_pct * n_mels\n",
    "    \n",
    "    for _ in range(n_freq_masks):\n",
    "        aug_spec = transforms.FrequencyMasking(freq_masks_param)(aug_spec, mask_value)\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "\n",
    "    for _ in range(n_time_masks):\n",
    "        aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def create_df():\n",
    "\n",
    "    file_path = []\n",
    "    class_name = []\n",
    "    \n",
    "    for i in glob.glob(\"./datasets/donateacry_corpus_cleaned_and_updated_data/*\"):\n",
    "        \n",
    "        if(os.path.isdir(i)):\n",
    "            \n",
    "            folder_base_name = os.path.basename(i)\n",
    "            \n",
    "            for file in glob.glob(i + \"/*.wav\"):\n",
    "\n",
    "                file_path.append(file)\n",
    "                class_name.append(folder_base_name)\n",
    "\n",
    "\n",
    "    return pd.DataFrame({'relative_path':file_path, 'class_name':class_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "\n",
    "class AudioUtil():\n",
    "    @staticmethod\n",
    "    def open(audio_file, sample_rate, mono_channel, use_librosa = True):\n",
    "        \n",
    "        if use_librosa == True:\n",
    "            sig, sr = librosa.load(audio_file, sr=sample_rate, mono=mono_channel, res_type='kaiser_best')\n",
    "            sig = torch.from_numpy(sig)\n",
    "\n",
    "            if(sig.dim() == 1):\n",
    "                sig = torch.unsqueeze(sig, dim=0)\n",
    "        else:\n",
    "            sig, sr = torchaudio.load(audio_file)\n",
    "\n",
    "        return (sig, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "import os\n",
    "import glob\n",
    "\n",
    "class SoundDS(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        ########################\n",
    "        self.use_librosa = True  \n",
    "        #######################\n",
    "\n",
    "        self.df = df\n",
    "        # self.duration = 4000\n",
    "        self.duration   = 7000            #miliseconds\n",
    "        self.sr         = 22050           #44100\n",
    "\n",
    "        self.num_channel = 1    \n",
    "                                   \n",
    "        self.shift_pct = 0.4\n",
    "        self.classes = [\"belly_pain\", \"burping\", \"discomfort\", \"hungry\", \"tired\"]\n",
    "                       \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        audio_file = self.df.loc[idx, 'relative_path']\n",
    "        class_name = self.df.loc[idx, 'class_name']\n",
    "        class_num = self.classes.index(class_name)\n",
    "\n",
    "        aud = AudioUtil.open(audio_file,\n",
    "                            sample_rate = self.sr,\n",
    "                            mono_channel = self.num_channel==1,\n",
    "                            use_librosa=self.use_librosa)\n",
    "\n",
    "        \n",
    "\n",
    "        if self.use_librosa == False:\n",
    "        \n",
    "            # reaud = resample(aud, self.sr)              \n",
    "            # Yukarıdaki fonksiyon gerekesizdir. Çünkü torchaudio.transforms.Resample()() fonksiyonu \n",
    "            # kendisine birden fazla kanallı ses verisi alıp, geriye frekansı düzenlenmiş aynı kanal sayısında\n",
    "            # örneği geri döndürmektedir.\n",
    "            # Bu yüzden yukarıdaki fonksiyon yerine aşağıdaki fonksiyon kullanılmalıdır\n",
    "            # Not: torchaudio.transforms sınıfı birden fazla kanallı ses verisi ile çalışabilmektedir.\n",
    "\n",
    "            aud = torchaudio.transforms.Resample(aud[1], self.sr)(aud), self.sr\n",
    "\n",
    "\n",
    "        rechan = rechannel(aud, self.num_channel)\n",
    "        # print(rechan[0].shape)\n",
    "\n",
    "        dur_aud = pad_trunc(rechan, self.duration)\n",
    "        # print(dur_aud[0].shape)\n",
    "\n",
    "        shift_aud = time_shift(dur_aud, self.shift_pct)\n",
    "        # print(shift_aud[0].shape)\n",
    "\n",
    "        sgram = spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "\n",
    "        aug_sgram = spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "        \n",
    "        return aug_sgram, class_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "df = create_df()\n",
    "\n",
    "myds = SoundDS( df )\n",
    "\n",
    "\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8) # 0.8\n",
    "num_val = num_items - num_train\n",
    "\n",
    "# val_ds:abbreviation for validation data set\n",
    "train_ds, val_ds = random_split(myds,[num_train, num_val])\n",
    "\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=8, shuffle=False) #bacth_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 64, 301])\n"
     ]
    }
   ],
   "source": [
    "it = iter(val_dl)\n",
    "s, c = it.next()\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.nn as nn\n",
    "\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class Classifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self, n_channel):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Third Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Fourth Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "\n",
    "        self.lin = nn.Linear(in_features=64, out_features=5)\n",
    "        \n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n",
    "\n",
    "# # Create the model and put it on the GPU if available\n",
    "# myModel = AudioClassifier()\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# myModel = myModel.to(device)\n",
    "\n",
    "# # Check that it is on Cuda\n",
    "# next(myModel.parameters()).device\n",
    "        \n",
    "\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = Classifier(n_channel=1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "myModel = myModel.to(device)\n",
    "\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATE A CONVOLUTIONAL MODEL CLASS\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channel):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        conv_layers = []\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "\n",
    "        # First convolution block\n",
    "        self.conv1 = nn.Conv2d(n_channel, 8, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
    "        # self.relu layer\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers.extend( [self.conv1, self.relu, self.bn1, self.dropout] )\n",
    "\n",
    "\n",
    "        # Second convolution block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
    "        # self.relu layer\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers.extend( [self.conv2, self.relu, self.bn2, self.dropout] )\n",
    "\n",
    "\n",
    "        # Third convolution block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
    "        #self.relu layer\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a= 0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers.extend( [self.conv3, self.relu, self.bn3, self.dropout] )\n",
    "\n",
    "        # Fourth convolution block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        #self.relu layer\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a = 0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers.extend( [self.conv4, self.relu, self.bn4, self.dropout] )\n",
    "\n",
    "\n",
    "        # 5. convolution block\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        #self.relu layer\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        init.kaiming_normal_(self.conv5.weight, a = 0.1)\n",
    "        self.conv5.bias.data.zero_()\n",
    "        conv_layers.extend( [self.conv5, self.relu, self.bn5, self.dropout] )\n",
    "\n",
    "\n",
    "        # 6. convolution block\n",
    "        self.conv6 = nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        #self.relu layer\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        init.kaiming_normal_(self.conv6.weight, a = 0.1)\n",
    "        self.conv6.bias.data.zero_()\n",
    "        conv_layers.extend( [self.conv6, self.relu, self.bn6, self.dropout] )\n",
    "\n",
    "\n",
    "        # 7. convolution block\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        #self.relu layer\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        init.kaiming_normal_(self.conv7.weight, a = 0.1)\n",
    "        self.conv7.bias.data.zero_()\n",
    "        conv_layers.extend( [self.conv7, self.relu, self.bn7, self.dropout] )\n",
    "\n",
    "\n",
    "\n",
    "        # Last step for classification\n",
    "        # Fully Connected Linear Layer\n",
    "\n",
    "        # Downsizing with pooling\n",
    "        self.avgPool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        \n",
    "        #flattening for input to linear layer\n",
    "        self.flatten = nn.Flatten(1, -1)\n",
    "\n",
    "        self.linear_layer1 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.linear_layer2 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.linear_layer3 = nn.Linear(in_features=64, out_features=16)\n",
    "        self.linear_layer4 = nn.Linear(in_features=16, out_features=5)\n",
    "    \n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        conv_layers.extend([self.avgPool, self.flatten,\n",
    "                            self.linear_layer1, self.dropout2,\n",
    "                            self.linear_layer2, self.dropout2,\n",
    "                            self.linear_layer3,\n",
    "                            self.linear_layer4])\n",
    "\n",
    "        #The list of conv_layer is unpacked and sent\n",
    "        self.conv = nn.Sequential(*conv_layers) # nn.Squential() gets *args or OrderedDict\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        return self.conv(inputs)\n",
    "\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = Classifier(n_channel=1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "myModel = myModel.to(device)\n",
    "\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.23, Accuracy: 0.62\n",
      "Epoch: 1, Loss: 0.73, Accuracy: 0.81\n",
      "Epoch: 2, Loss: 0.71, Accuracy: 0.81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\adam\\GitHub\\cradle-model-extractor\\Cry_Classification.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 175>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m         ay\u001b[39m.\u001b[39mgrid(\u001b[39mTrue\u001b[39;00m, linewidth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m num_epochs\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m training(myModel, train_dl, num_epochs)\n",
      "\u001b[1;32mc:\\Users\\adam\\GitHub\\cradle-model-extractor\\Cry_Classification.ipynb Cell 16\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(model, train_dl, num_epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mLearning Rate\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m], epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Repeat for each batch in the training set\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dl):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39mif\u001b[39;00m(epoch\u001b[39m*\u001b[39mnum_batches\u001b[39m+\u001b[39mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m\u001b[39m4\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m         \u001b[39m# scheduler.step()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;32mc:\\Users\\adam\\GitHub\\cradle-model-extractor\\Cry_Classification.ipynb Cell 16\u001b[0m in \u001b[0;36mSoundDS.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m class_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mloc[idx, \u001b[39m'\u001b[39m\u001b[39mclass_name\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m class_num \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses\u001b[39m.\u001b[39mindex(class_name)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m aud \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39;49mopen(audio_file,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m                     sample_rate \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                     mono_channel \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_channel\u001b[39m==\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                     use_librosa\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_librosa)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_librosa \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m# reaud = resample(aud, self.sr)              \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39m# Bu yüzden yukarıdaki fonksiyon yerine aşağıdaki fonksiyon kullanılmalıdır\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# Not: torchaudio.transforms sınıfı birden fazla kanallı ses verisi ile çalışabilmektedir.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     aud \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mResample(aud[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msr)(aud), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msr\n",
      "\u001b[1;32mc:\\Users\\adam\\GitHub\\cradle-model-extractor\\Cry_Classification.ipynb Cell 16\u001b[0m in \u001b[0;36mAudioUtil.open\u001b[1;34m(audio_file, sample_rate, mono_channel, use_librosa)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(audio_file, sample_rate, mono_channel, use_librosa \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m use_librosa \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         sig, sr \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mload(audio_file, sr\u001b[39m=\u001b[39;49msample_rate, mono\u001b[39m=\u001b[39;49mmono_channel, res_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mkaiser_best\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         sig \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(sig)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adam/GitHub/cradle-model-extractor/Cry_Classification.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39mif\u001b[39;00m(sig\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\util\\decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[0;32m     87\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     91\u001b[0m args_msg \u001b[39m=\u001b[39m [\n\u001b[0;32m     92\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, arg)\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[39m-\u001b[39mextra_args:])\n\u001b[0;32m     94\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:179\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    176\u001b[0m     y \u001b[39m=\u001b[39m to_mono(y)\n\u001b[0;32m    178\u001b[0m \u001b[39mif\u001b[39;00m sr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     y \u001b[39m=\u001b[39m resample(y, orig_sr\u001b[39m=\u001b[39;49msr_native, target_sr\u001b[39m=\u001b[39;49msr, res_type\u001b[39m=\u001b[39;49mres_type)\n\u001b[0;32m    181\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     sr \u001b[39m=\u001b[39m sr_native\n",
      "File \u001b[1;32mc:\\Users\\adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\util\\decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[0;32m     87\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     91\u001b[0m args_msg \u001b[39m=\u001b[39m [\n\u001b[0;32m     92\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, arg)\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[39m-\u001b[39mextra_args:])\n\u001b[0;32m     94\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:647\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[0;32m    645\u001b[0m     y_hat \u001b[39m=\u001b[39m soxr\u001b[39m.\u001b[39mresample(y\u001b[39m.\u001b[39mT, orig_sr, target_sr, quality\u001b[39m=\u001b[39mres_type)\u001b[39m.\u001b[39mT\n\u001b[0;32m    646\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 647\u001b[0m     y_hat \u001b[39m=\u001b[39m resampy\u001b[39m.\u001b[39;49mresample(y, orig_sr, target_sr, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49mres_type, axis\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    649\u001b[0m \u001b[39mif\u001b[39;00m fix:\n\u001b[0;32m    650\u001b[0m     y_hat \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mfix_length(y_hat, size\u001b[39m=\u001b[39mn_samples, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\resampy\\core.py:149\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(x, sr_orig, sr_new, axis, filter, parallel, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m     resample_f_p(\n\u001b[0;32m    140\u001b[0m         x\u001b[39m.\u001b[39mswapaxes(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis),\n\u001b[0;32m    141\u001b[0m         t_out,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m         y\u001b[39m.\u001b[39mswapaxes(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis),\n\u001b[0;32m    147\u001b[0m     )\n\u001b[0;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     resample_f_s(\n\u001b[0;32m    150\u001b[0m         x\u001b[39m.\u001b[39;49mswapaxes(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, axis),\n\u001b[0;32m    151\u001b[0m         t_out,\n\u001b[0;32m    152\u001b[0m         interp_win,\n\u001b[0;32m    153\u001b[0m         interp_delta,\n\u001b[0;32m    154\u001b[0m         precision,\n\u001b[0;32m    155\u001b[0m         scale,\n\u001b[0;32m    156\u001b[0m         y\u001b[39m.\u001b[39;49mswapaxes(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, axis),\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\np\\ufunc\\gufunc.py:192\u001b[0m, in \u001b[0;36mGUFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd(sig)\n\u001b[0;32m    191\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_ufunc()\n\u001b[1;32m--> 192\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mufunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "from distutils.fancy_getopt import wrap_text\n",
    "from sched import scheduler\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ignite.handlers.param_scheduler import create_lr_scheduler_with_warmup\n",
    "\n",
    "writer = SummaryWriter(\"runs/Cry_Classification/writer\")\n",
    "writer2 = SummaryWriter(\"runs/Cry_Classification/writer2\")\n",
    "\n",
    "if not os.path.exists('./check_points_for_classification'):\n",
    "    os.mkdir('./check_points_for_classification')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "def training(model:nn.Module, train_dl, num_epochs):\n",
    "    # Loss Function, Optimizer and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "    #this is original scheduler and not a good for this model and dataset\n",
    "    # torch_lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.05,\n",
    "    #                                                 steps_per_epoch=int(len(train_dl)),\n",
    "    #                                                 epochs=num_epochs,\n",
    "    #                                                 anneal_strategy='linear')\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer,\n",
    "                                                                gamma=0.5)\n",
    "\n",
    "    # scheduler = create_lr_scheduler_with_warmup(torch_lr_scheduler,\n",
    "    #                                            warmup_start_value=0.0,\n",
    "    #                                            warmup_end_value=0.005,\n",
    "    #                                            warmup_duration=7)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer,\n",
    "    #                                                                  T_0=3,\n",
    "    #                                                                  T_mult=2,\n",
    "    #                                                                  eta_min=0.05)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.LinearLR(optimizer=optimizer,\n",
    "    #                                               )\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    num_batches = len(train_dl)\n",
    "    # scheduler(None)\n",
    "\n",
    "    # Repeat for each epoch\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "\n",
    "        sum_loss_every_4_batch = 0\n",
    "        sum_correct_prediction_4_batch = 0\n",
    "        sum_total_prediction_4_batch = 0\n",
    "        \n",
    "        writer.add_scalar(\"Learning Rate\", optimizer.param_groups[0]['lr'], epoch)\n",
    "       \n",
    "        # Repeat for each batch in the training set\n",
    "        for i, data in enumerate(train_dl):\n",
    "            if(epoch*num_batches+i+1)%4 == 0:\n",
    "\n",
    "                # scheduler.step()\n",
    "                pass\n",
    "            \n",
    "            count+=1\n",
    "\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            \n",
    "\n",
    "            # Normalize the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # torch_lr_scheduler.step()\n",
    "\n",
    "            losses.append(loss.cpu().data.numpy())\n",
    "            \n",
    "\n",
    "            # Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "            sum_loss_every_4_batch += loss.item()\n",
    "            \n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "            sum_correct_prediction_4_batch += (prediction == labels).sum().item()\n",
    "            sum_total_prediction_4_batch += prediction.shape[0]\n",
    "\n",
    "            # writer.add_scalar(\"Learning Rate\", scheduler.get_last_lr()[0], epoch*num_batches+i+1)  \n",
    "            # writer.add_scalar(\"Learning Rate\", optimizer.param_groups[0]['lr'], count) \n",
    "\n",
    "            if (i+1) % 4 == 0:    # print every 10 mini-batches\n",
    "                avg_loss = sum_loss_every_4_batch/4\n",
    "                avg_acc = sum_correct_prediction_4_batch/sum_total_prediction_4_batch\n",
    "\n",
    "                #  print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "                writer.add_scalar('trainin loss - every 4 batch', avg_loss, epoch*num_batches+i+1)\n",
    "                writer.add_scalar('accuracy - every 4 batch', avg_acc, epoch*num_batches+i+1)  \n",
    "\n",
    "                sum_loss_every_4_batch = 0\n",
    "                sum_correct_prediction_4_batch = 0\n",
    "                sum_total_prediction_4_batch = 0\n",
    "\n",
    "        scheduler.step()\n",
    "                \n",
    "    \n",
    "\n",
    "        # Print stats at the end of the epoch\n",
    "        \n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction/total_prediction\n",
    "\n",
    "        writer2.add_scalar('training loss - end of every epoch', avg_loss, (epoch+1)*num_batches)\n",
    "        writer2.add_scalar('accuracy - of end every epoch', acc, (epoch+1)*num_batches)\n",
    "\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "        #save checkpoints after every epoch\n",
    "\n",
    "        path = './check_points_for_classification/checkpoint_'+str(epoch+1)+'.pth'\n",
    "\n",
    "        checkpoint = {\"model_state\": model.state_dict(),\n",
    "                      \"optim_state\": optimizer.state_dict(),\n",
    "                    #   \"scheduler\"  : scheduler.state_dict()\n",
    "                      }\n",
    "\n",
    "        torch.save(checkpoint, path)\n",
    "\n",
    "    writer.close()\n",
    "    writer2.close()\n",
    "\n",
    "    if True:\n",
    "        plt.plot([x for x in range(1,count+1)], losses)\n",
    "        plt.xticks(np.arange(1,count+1, 99))\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Train Number\")\n",
    "        ay = plt.twiny()\n",
    "        ay.set_xlabel(\"Epoch Number\")\n",
    "        ay.set_xticks(np.arange(1, num_epochs+1, 1))\n",
    "\n",
    "        y = [0 for x in range(1, num_epochs)]\n",
    "        epoch = [x for x in range(1, num_epochs)]\n",
    "        ay.plot(epoch,y, color=\"white\")\n",
    "        \n",
    "        ay.grid(True, linewidth=2)\n",
    "\n",
    "\n",
    "num_epochs=12\n",
    "\n",
    "training(myModel, train_dl, num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1 Loss: 1.21 Accuracy: 0.77, Total items: 91\n",
      "model_2 Loss: 0.70 Accuracy: 0.86, Total items: 91\n",
      "model_3 Loss: 0.61 Accuracy: 0.85, Total items: 91\n",
      "model_4 Loss: 0.60 Accuracy: 0.86, Total items: 91\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "writer = SummaryWriter('runs/Cry_ClassificationTest/writer_epoch')\n",
    "writer_batch = SummaryWriter('runs/Cry_ClassificationTest/writer_batch')\n",
    "\n",
    "def inference (model:nn.Module, val_dl, model_no):\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    total_loss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Disable gradient updates\n",
    "    with torch.no_grad():\n",
    "\n",
    "        runningLoss = 0.0\n",
    "        runningPredict  = 0\n",
    "        runnigCorrectPredict = 0\n",
    "\n",
    "        for i, data in enumerate(val_dl):\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Normalize the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # Get predictions\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "            total_loss += loss\n",
    "\n",
    "            runnigCorrectPredict += (prediction == labels).sum().item()\n",
    "            runningPredict       += prediction.shape[0]\n",
    "            runningLoss += loss\n",
    "            \n",
    "            n = 1\n",
    "            if i%n==0:\n",
    "                writer_batch.add_scalar(\"model_\"+str(model_no)+\" accuracy every n_batch\", runnigCorrectPredict/runningPredict, i+1)\n",
    "                writer_batch.add_scalar(\"model_\"+str(model_no)+\" loss every n_batch\", runningLoss/(n), i+1)\n",
    "                runningLoss = 0.0\n",
    "                runningPredict  = 0\n",
    "                runnigCorrectPredict = 0\n",
    "    \n",
    "    acc = correct_prediction/total_prediction\n",
    "    \n",
    "    print(\"model_\"+str(model_no)+ f' Loss: {total_loss/len(val_dl):.2f} Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "\n",
    "# Run inference on trained model with the validation set\n",
    "\n",
    "models = []\n",
    "model_x = Classifier(n_channel=1).to(device)\n",
    "\n",
    "for model_no in range(1,5):\n",
    "    path = 'check_points_for_classification/checkpoint_'+str(model_no)+'.pth'\n",
    "    model_x.load_state_dict( torch.load(path)[\"model_state\"] )\n",
    "    models.append(copy.deepcopy(model_x))\n",
    "\n",
    "\n",
    "for i in range(1,5):\n",
    "    inference(models[i-1], val_dl, model_no = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "\n",
    "import torch.utils.tensorboard\n",
    "\n",
    "def inference_one (model, data):\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Disable gradient updates\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Get the input features and target labels, and put them on the GPU\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Normalize the inputs\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "        \n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "\n",
    "# Run inference on trained model with the validation set\n",
    "inference(myModel, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DENEME DENEME DENEME\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "aud = torchaudio.load(\"test.wav\")\n",
    "\n",
    "x, sr = aud\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "librosa.display.waveshow(aud[0].numpy())\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "\n",
    "reaud = resample(aud, sr)\n",
    "rechan = rechannel(reaud, 2)\n",
    "\n",
    "dur_aud = pad_trunc(rechan, 5000)\n",
    "shift_aud = time_shift(dur_aud, 0.4)\n",
    "sgram = spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "aug_sgram = spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "# print(dur_aud[0])\n",
    "librosa.display.waveshow(shift_aud[0].numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9976)\n",
      "tensor([-0.3067,  0.0706, -0.3059,  0.8448, -0.3060])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([-6.8571e-01,  1.5788e-01, -6.8396e-01,  1.8886e+00, -6.8400e-01])\n",
    "\n",
    "s = 0\n",
    "for i in a:\n",
    "    s += i*i\n",
    "print(s)\n",
    "\n",
    "b = torch.nn.functional.normalize(a, dim=0)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ecc031d86696929f30629648f7816eb3c79c248c1db74fa715074cbd38220b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
